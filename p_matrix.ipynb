{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Lasso\n",
    "# fit a logistic regression model on an imbalanced classification dataset\n",
    "from sklearn.metrics import r2_score, roc_auc_score, recall_score, precision_score, accuracy_score, mean_squared_error, auc, roc_curve\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import scipy.stats as st\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "from matplotlib.transforms import Affine2D\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import dimod\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'todel'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory structure\n",
    "models_dir = \"models\"\n",
    "models_evaluation_dir = \"models_evaluation\"\n",
    "plots_dir = \"plots\"\n",
    "extended_model_base_dir = os.path.join(models_dir, \"extended_models\")\n",
    "extended_model_evaluation_base_dir = os.path.join(models_evaluation_dir, \"extended_models\")\n",
    "condensed_model_base_dir = os.path.join(models_dir, \"condensed_models\")\n",
    "condensed_model_evaluation_base_dir = os.path.join(models_evaluation_dir, \"condensed_models\")\n",
    "p_matrix_base_dir = os.path.join(models_evaluation_dir, \"p_matrix\")\n",
    "p_matrix_plot_base_dir = os.path.join(plots_dir,\"p_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv(\"data/med_orginal2.csv\",header=0, sep=\"\\,\").fillna(method = \"ffill\")\n",
    "\n",
    "\n",
    "target_f = 'Graft loss 1 year'\n",
    "\n",
    "mandatory_f = [\n",
    "    'AKI - KDIGO 2012',\n",
    "    'FSGS',\n",
    "    'Reduction to steroid only',\n",
    "    'Transfusion [YES/NO]',\n",
    "]\n",
    "\n",
    "with open(\"data/16.json\") as file:\n",
    "    miqubo_result = dimod.SampleSet.from_serializable(json.load(file))\n",
    "    miqubo_f = [x for x, y in miqubo_result.first.sample.items() if y ==1]\n",
    "\n",
    "miqubo_f = list(set(miqubo_f) - set(mandatory_f) - set(target_f))\n",
    "assert sum(df_all.isna().sum())==0, \"Still nan entries in dataset\"\n",
    "\n",
    "df_ = deepcopy(df_all)\n",
    "naming_map_ = {x[1] : \"f_{}\".format(x[0]) for x in enumerate(df_.columns)}\n",
    "df_ = df_.rename(columns=naming_map_)\n",
    "target_f = naming_map_[target_f]\n",
    "mandatory_f = [naming_map_[x] for x in mandatory_f]\n",
    "free_f_miqubo = [naming_map_[x] for x in miqubo_f]\n",
    "free_f_all_ = list(set(naming_map_.values()) - set(mandatory_f) - set(target_f))\n",
    "\n",
    "# get raw dataset\n",
    "df_all = df_all.rename(columns = naming_map_)\n",
    "df_miqubo = df_all[[target_f] + mandatory_f + free_f_miqubo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(df): \n",
    "    binary_f = [x for x in df.columns if len(np.unique(df[x]))==2]\n",
    "    float_f = list(set([x for x, y in df.dtypes.items() if y == np.float]) - set(binary_f))\n",
    "    int_f = list(set([x for x, y in df.dtypes.items() if y == np.int]) - set(binary_f))\n",
    "\n",
    "    df_float_norm = pd.DataFrame(data=preprocessing.StandardScaler().fit(df[float_f]).transform(df[float_f]), columns=float_f)\n",
    "    df_int_norm = pd.DataFrame(data=preprocessing.StandardScaler().fit(df[int_f]).transform(df[int_f]), columns=int_f)\n",
    "    return pd.concat([df[binary_f], df_float_norm, df_int_norm], axis=1, join=\"inner\")\n",
    "\n",
    "def sample_dataset(df, target_f):\n",
    "    # get normalized dataset\n",
    "    y_train = df.pop(target_f)\n",
    "    x_train = df\n",
    "\n",
    "    # get normalized and sampled dataset\n",
    "    df_sampled, label_sampled = SMOTE().fit_resample(x_train, y_train)\n",
    "    df_sampled.insert(0, target_f, label_sampled)\n",
    "    return df_sampled\n",
    "\n",
    "def transform_dataset(df, target_f = target_f, normalized=True, sampled=True):\n",
    "    df_ = deepcopy(df)\n",
    "    if normalized:\n",
    "        df_ = normalize_dataset(df_)\n",
    "    if sampled:\n",
    "        df_ = sample_dataset(df_, target_f)\n",
    "\n",
    "    y_train = df_.pop(target_f)\n",
    "    x_train = df_\n",
    "    return train_test_split(x_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "   \n",
    "\n",
    "x_train_raw, x_test_raw, y_train_raw, y_test_raw = transform_dataset(df_all)\n",
    "x_train_raw, x_test_raw, y_train_raw, y_test_raw = transform_dataset(df_all, target_f, normalized = False, sampled=True)\n",
    "x_train_raw, x_test_raw, y_train_raw, y_test_raw = transform_dataset(df_all, target_f, normalized = True, sampled=False)\n",
    "x_train_raw, x_test_raw, y_train_raw, y_test_raw = transform_dataset(df_all, target_f, normalized = True, sampled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, models, models_names, target_f, miqubo = True, normalized = True, sampled=True):\n",
    "    miqubo_label = \"miqubo\" if miqubo else \"all\"\n",
    "    model_save_dir = \"raw\"\n",
    "    if normalized:\n",
    "        df = normalize_dataset(df)\n",
    "    if sampled:\n",
    "        df = sample_dataset(df, target_f)\n",
    "\n",
    "    if sampled:\n",
    "        if normalized: \n",
    "            model_save_dir = \"sampled_normalized\"\n",
    "        else:\n",
    "            model_save_dir = \"sampled\"\n",
    "    else:\n",
    "        if normalized:\n",
    "            model_save_dir = \"normalized\"\n",
    "    try: \n",
    "        os.mkdir(os.path.join(models_dir, model_save_dir)) \n",
    "    except OSError as error: \n",
    "        print(error) \n",
    "\n",
    "    try: \n",
    "        os.mkdir(os.path.join(models_evaluation_dir, model_save_dir)) \n",
    "    except OSError as error: \n",
    "        print(error) \n",
    "\n",
    "    y_train = df.pop(target_f)\n",
    "    x_train = df\n",
    "    x_train, x_test, y_train, y_test =train_test_split(x_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "    trained_models = {}\n",
    "    threashold = 0.9\n",
    "\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    for model, model_name in zip(models, models_names):\n",
    "        # evaluate model\n",
    "        scores = cross_val_score(model, x_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        model_path = os.path.join(models_dir, model_save_dir, '{}_{}.joblib'.format(model_name, miqubo_label))\n",
    "        pickle.dump(\n",
    "            model, \n",
    "            open(model_path, 'wb')\n",
    "            )\n",
    "        # discretize predictions\n",
    "        y_pred = np.where(model.predict(x_test)>threashold,1, 0)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        roc = roc_auc_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred) \n",
    "        precission = precision_score(y_test, y_pred) \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, model.predict(x_test))\n",
    "\n",
    "        # add model to dictionary\n",
    "        trained_models[model_name] = {}\n",
    "        trained_models[model_name][\"model\"] = model\n",
    "        trained_models[model_name][\"name\"] = model_name\n",
    "        trained_models[model_name][\"pred\"] = model.predict(x_test)\n",
    "        trained_models[model_name][\"predT\"] =y_pred\n",
    "        trained_models[model_name][\"mse\"] = mse\n",
    "        trained_models[model_name][\"r2\"] = r2\n",
    "        trained_models[model_name][\"roc\"] = roc\n",
    "        trained_models[model_name][\"recall\"] = recall\n",
    "        trained_models[model_name][\"precission\"] = precission\n",
    "        trained_models[model_name][\"accurarcy\"] = accuracy\n",
    "        trained_models[model_name][\"fpr\"] = fpr\n",
    "        trained_models[model_name][\"tpr\"] = tpr \n",
    "        trained_models[model_name][\"thresholds\"] = thresholds\n",
    "\n",
    "        # summarize performance\n",
    "        #print('Mean ROC AUC: %.3f' % mean(scores))\n",
    "        #print(\"Name: {} | ROC: {}, recall: {}, precission: {}, accuracy: {}\".format(name, roc, recall, precission, accuracy))\n",
    "\n",
    "    with open(os.path.join(\n",
    "        models_evaluation_dir, model_save_dir,\n",
    "        '{}.csv'.format(miqubo_label)), \n",
    "        'w', encoding='UTF8') as f:\n",
    "        # create the csv writer\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # write the header\n",
    "        writer.writerow(['name', 'mse', 'r2', 'roc', 'recall', 'precission', 'accurarcy'])\n",
    "\n",
    "        for model in trained_models:\n",
    "            model_evaluation = trained_models[model_name]\n",
    "            row = [\n",
    "                model_evaluation['name'],\n",
    "                model_evaluation['mse'],\n",
    "                model_evaluation['r2'],\n",
    "                model_evaluation['roc'],\n",
    "                model_evaluation['recall'],\n",
    "                model_evaluation['precission'],\n",
    "                model_evaluation['accurarcy'],\n",
    "            ]\n",
    "            # write the data\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'models/sampled'\n",
      "[Errno 17] File exists: 'models_evaluation/sampled'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/kernels.py:418: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'models/raw'\n",
      "[Errno 17] File exists: 'models_evaluation/raw'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'models/sampled'\n",
      "[Errno 17] File exists: 'models_evaluation/sampled'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/kernels.py:418: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'models/normalized'\n",
      "[Errno 17] File exists: 'models_evaluation/normalized'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'models/sampled_normalized'\n",
      "[Errno 17] File exists: 'models_evaluation/sampled_normalized'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "models_names = [\"Lasso\", \"Enet\", \"Loger\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "        \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "        \"Naive Bayes\", \"QDA\"]\n",
    "alpha = 0.01\n",
    "models = [\n",
    "    linear_model.Lasso(alpha=alpha, normalize = False),\n",
    "    linear_model.ElasticNet(alpha=alpha, l1_ratio=0.7, normalize = False),\n",
    "    LogisticRegression(solver='lbfgs', class_weight={x:y for x, y in zip(np.unique(df_all[target_f]), np.bincount(df_all[target_f]))}),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "miqubo_trained_models_sampled            = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = True,  normalized = False, sampled = True)\n",
    "miqubo_trained_models                    = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = True,  normalized = False, sampled = False)\n",
    "miqubo_trained_models_normalized         = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = True,  normalized = True,  sampled = False)\n",
    "miqubo_trained_models_sampled_normalized = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = True,  normalized = True,  sampled = True)\n",
    "all_trained_models                       = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = False, normalized = False, sampled = False)\n",
    "all_trained_models_sampled               = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = False, normalized = False, sampled = True)\n",
    "all_trained_models_normalized            = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = False, normalized = True,  sampled = False)\n",
    "all_trained_models_sampled_normalized    = train_models(deepcopy(df_all), models, models_names, target_f,miqubo = False, normalized = True,  sampled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "\n",
    "# Model A (random) vs. \"good\" model B\n",
    "#preds_A = np.array([.5, .5, .5, .5, .5, .5, .5, .5, .5, .5])\n",
    "#preds_B = np.array([.2, .5, .1, .4, .9, .8, .7, .5, .9, .8])\n",
    "#actual= np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1])\n",
    "\n",
    "def group_preds_by_label(preds, actual):\n",
    "    X = [p for (p, a) in zip(preds, actual) if a]\n",
    "    Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "    return X, Y\n",
    "\n",
    "def get_p_value(preds_A, preds_B, actual_A, actual_B):\n",
    "    X_A, Y_A = group_preds_by_label(preds_A, actual_A)\n",
    "    X_B, Y_B = group_preds_by_label(preds_B, actual_B)\n",
    "    V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "    auc_A = auc(X_A, Y_A)\n",
    "    auc_B = auc(X_B, Y_B)\n",
    "    # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "    var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "    var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "            + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "    covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "                + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "    # Two tailed test\n",
    "    z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "    p = st.norm.sf(abs(z))*2\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_matrix(df, target_f, trained_models_a, trained_models_b, miqubo_a = True, miqubo_b = True, normalized_a = True, normalized_b = True, sampled_a=True, sampled_b=True):\n",
    "    df_a = deepcopy(df)\n",
    "    df_b = deepcopy(df)\n",
    "    if normalized_a:\n",
    "        df_a = normalize_dataset(df_a)\n",
    "    if sampled_a:\n",
    "        df_a = sample_dataset(df_a, target_f)\n",
    "    if normalized_b:\n",
    "        df_b = normalize_dataset(df_b)\n",
    "    if sampled_a:\n",
    "        df_b = sample_dataset(df_b, target_f)\n",
    "\n",
    "    y_train_a = df_a.pop(target_f)\n",
    "    x_train_a = df_a\n",
    "    x_train, x_test_a, y_train, y_test_a = train_test_split(x_train_a, y_train_a, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    label_a = \"Miqubo\" if miqubo_a else \"All\"\n",
    "    label_a += \" features over \"\n",
    "    \n",
    "    if sampled_a:\n",
    "        if normalized_a: \n",
    "            label_a = \"sampled and normalized\"\n",
    "        else:\n",
    "            label_a = \"sampled\"\n",
    "    else:\n",
    "        if normalized_a:\n",
    "            label_a = \"normalized\"\n",
    "    label_a += \" dataset\"\n",
    "\n",
    "    label_b = \"Miqubo\" if miqubo_a else \"All\"\n",
    "    label_b += \" features over \"\n",
    "    \n",
    "    if sampled_b:\n",
    "        if normalized_b: \n",
    "            label_b = \"sampled and normalized\"\n",
    "        else:\n",
    "            label_b = \"sampled\"\n",
    "    else:\n",
    "        if normalized_b:\n",
    "            label_b = \"normalized\"\n",
    "    label_b += \" dataset\"\n",
    "\n",
    "    \n",
    "    n_models = len(trained_models_a)\n",
    "    p_matrix = np.zeros((n_models, n_models))\n",
    "    for row, model_A in enumerate(trained_models_a):\n",
    "        preds_A = trained_models_a[model_A]['model'].predict(x_test_a)\n",
    "        for col, model_B in enumerate(trained_models_b):\n",
    "            preds_B = trained_models_b[model_B]['model'].predict(x_test_b)\n",
    "            p_matrix[row][col] = get_p_value(preds_A, preds_B, y_test_a, y_test_b)\n",
    "    \n",
    "    # save p_matrix to csv\n",
    "    np.savetxt(os.path.join(p_matrix_base_dir, file_name+\".csv\"), p_matrix, delimiter = ',')\n",
    "\n",
    "    # save\n",
    "\n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
